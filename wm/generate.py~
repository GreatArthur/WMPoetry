from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import copy
import tensorflow as tf
import cPickle
from generate_base import Generator
from model import PoemModel
from tool import PoetryTool
from config import hps
import numpy as np

class GeneratorUI(object):

    def __init__(self):
         # Construct hyper-parameter
        self.hps = hps
        self.tool = PoetryTool(sens_num=hps.sens_num,
            key_slots=hps.key_slots, enc_len=hps.bucket[0],
            dec_len=hps.bucket[1])
        # If there isn't a pre-trained word embedding, just
        # set it to None, then the word embedding
        # will be initialized with a norm distribution.
        if hps.init_emb == '':
            self.init_emb = None
        else:
            self.init_emb = np.load(self.hps.init_emb)
            print ("init_emb_size: %s" % str(np.shape(self.init_emb)))
        self.tool.load_dic(hps.vocab_path, hps.ivocab_path)

        vocab_size = self.tool.get_vocab_size()
        assert vocab_size > 0
        PAD_ID = self.tool.get_PAD_ID()
        assert PAD_ID > 0

        self.hps = self.hps._replace(vocab_size=vocab_size, PAD_ID=PAD_ID)


        self.tool = PoetryTool()
        self.__loadPatterns()


    def __loadPatterns(self):
        fin = open("data/other/GLPatterns.txt", 'r')
        lines = fin.readlines()
        fin.close()
        self.patterns = []
        for line in lines:
            line = line.strip()
            para = line.split("#")
            pas = para[3].split("|")
            newpas = []
            for pa in pas:
                pa = pa.split(" ")
                newpas.append([int(p) for p in pa])
            self.patterns.append((para[2], newpas))

    def load_dic(self, file_dir):
        """
        loading  training data, including vocab, inverting vocab and corpus
        """
        vocab_file = open(file_dir + '/vocab.pkl', 'rb')
        dic = cPickle.load(vocab_file)
        vocab_file.close()

        ivocab_file = open(file_dir + '/ivocab.pkl', 'rb')
        idic = cPickle.load(ivocab_file)
        ivocab_file.close()

        return dic, idic

    def load_model(self, session, beam_size):
        """load parameters in session."""
        decode_hps = self.hps._replace(batch_size=beam_size)
        model = PoemModel(decode_hps)
        ckpt = tf.train.get_checkpoint_state("model/")
        if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):
            print("Reading model parameters from %s" %
                  ckpt.model_checkpoint_path)
            model.saver.restore(session, ckpt.model_checkpoint_path)
        else:
            raise ValueError("%s not found! " % ckpt.model_checkpoint_path)

        return model

    def load_model_by_path(self, session, beam_size, modefile):
        """load parameters in session."""
        decode_hps = self.hps._replace(batch_size=beam_size)
        model = PoemModel(decode_hps)

        if tf.gfile.Exists(modefile):
            print("Reading model parameters from %s" %modefile)
            model.saver.restore(session, modefile)
        else:
            raise ValueError("%s not found! " % modefile)

        return model

    def generate_one(self):
        beam_size = input("please input beam size>")
        self.sess = tf.InteractiveSession()
        self.model = self.load_model(self.sess, beam_size)
        self.generator = Generator(
            self.vocab, self.ivocab, self.hps, self.model, self.sess)

        while True:
            keys = raw_input("please input keywords (with whitespace split)> ")
            pattern_id = input("please select gl pattern > ")
            pattern_id -= 1
            yun = input("please input yun type ([1, 33]])> ")
            pattern = copy.deepcopy(self.patterns[pattern_id][1])
            print ("select pattern: %s" % (self.patterns[pattern_id][0]))
            ans, info = self.generator.generate_one(keys, pattern, yun, beam_size, True)
            if len(ans) == 0:
                print("generation failed!")
                print(info)
                continue

            for i, sen in enumerate(ans):
                print(sen)

    def modiPattern(self, patternStr):
        #print (patternStr)
        patterns = []
        for pstr in patternStr:
            pas = pstr.split(" ")
            pas = [int(pa) for pa in pas]
            patterns.append(pas)

        #print (patterns)
        return patterns

    
    def generate_specify_file(self, infile, outfile, beam_size, modefile = None):
        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.45)
        self.sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))
        if modefile is None:
            self.model = self.load_model(self.sess, beam_size)
        else:
            self.model = self.load_model_by_path(self.sess, beam_size, modefile)
        self.generator = Generator(
            self.vocab, self.ivocab, self.hps, self.model, self.sess)

        fin = open(infile, 'r')
        lines = fin.readlines()
        fin.close()

        fout = open(outfile, 'w')
        for i, line in enumerate(lines):
            line = line.strip()
            para = line.split("|")
            wstr = para[0].strip()
            print ("%d  keys: %s" % (i, wstr))
            pattern = self.modiPattern(para[1:])

            sens, info = self.generator.generate_specify(wstr, pattern, beam_size)
            if len(sens) == 0:
                fout.write(info + "\n")
            else:
                fout.write("|".join(sens) + "\n")
            fout.flush()

        fout.close()

    def modiSens(self, senstr):
        lines = senstr.split("|")
        sens = []
        for line in lines:
            sen = self.tool.lineSplit2list(line)
            sens.append(sen)

        return sens

def main(_):
    ui = GeneratorUI()
    #ui.generate_specify_file("test_inp_tang.txt", "test_out.txt", 20)
    ui.generate_one()


if __name__ == "__main__":
    tf.app.run()
